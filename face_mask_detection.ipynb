{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üò∑ Face Mask Detection ‚Äî MobileNetV2 Transfer Learning (PyTorch)\n",
                "\n",
                "A complete pipeline for detecting whether a person is wearing a face mask:\n",
                "\n",
                "| Step | Description |\n",
                "|------|-------------|\n",
                "| 1 | **Dataset** ‚Äî Synthetic face images (or real dataset) |\n",
                "| 2 | **Pre-process** ‚Äî Resize, normalise, augment |\n",
                "| 3 | **Transfer Learning** ‚Äî MobileNetV2 ImageNet base + custom head |\n",
                "| 4 | **Training** ‚Äî Adam + EarlyStopping |\n",
                "| 5 | **Evaluation** ‚Äî Accuracy, loss curves, confusion matrix |\n",
                "| 6 | **Webcam Demo** ‚Äî Real-time Haar-cascade + model inference |"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 0 ¬∑ Install Dependencies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import subprocess, sys\n",
                "\n",
                "# PyTorch (CPU build ‚Äî no path-length issues on Windows)\n",
                "subprocess.run([\n",
                "    sys.executable, '-m', 'pip', 'install',\n",
                "    'torch', 'torchvision',\n",
                "    '--index-url', 'https://download.pytorch.org/whl/cpu', '-q'\n",
                "], check=False)\n",
                "\n",
                "# Other dependencies\n",
                "for pkg in ['opencv-python', 'numpy', 'matplotlib', 'scikit-learn', 'pillow', 'seaborn']:\n",
                "    subprocess.run([sys.executable, '-m', 'pip', 'install', pkg, '-q'], check=False)\n",
                "\n",
                "print('‚úÖ Dependencies ready')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1 ¬∑ Imports"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os, time, random, warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from PIL import Image, ImageDraw\n",
                "\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.metrics         import classification_report, confusion_matrix\n",
                "\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "from torch.utils.data       import Dataset, DataLoader\n",
                "from torch.optim            import Adam\n",
                "from torch.optim.lr_scheduler import StepLR\n",
                "import torchvision.transforms as T\n",
                "from torchvision.models      import mobilenet_v2, MobileNet_V2_Weights\n",
                "\n",
                "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "\n",
                "print(f'PyTorch     : {torch.__version__}')\n",
                "print(f'Device      : {DEVICE}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2 ¬∑ Generate Synthetic Dataset\n",
                "\n",
                "> **Using a real dataset?**  \n",
                "> Download from https://github.com/chandrikadeb7/Face-Mask-Detection  \n",
                "> and place images in `dataset/with_mask/` and `dataset/without_mask/`  \n",
                "> then skip this cell.\n",
                ">\n",
                "> The cell below creates a **synthetic** dataset of drawn faces so the notebook  \n",
                "> runs fully offline in seconds."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "DATASET_DIR = 'dataset'\n",
                "N_IMAGES    = 300        # per class ‚Äì increase for better accuracy\n",
                "IMG_SIZE    = 224\n",
                "CATEGORIES  = ['with_mask', 'without_mask']\n",
                "\n",
                "\n",
                "def make_face(mask: bool, idx: int) -> Image.Image:\n",
                "    rng = np.random.default_rng(seed=idx)\n",
                "    bg  = tuple(int(c) for c in rng.integers(180, 255, 3))\n",
                "    img = Image.new('RGB', (100, 100), bg)\n",
                "    d   = ImageDraw.Draw(img)\n",
                "    skin = tuple(int(c) for c in (rng.integers(180,220), rng.integers(140,180), rng.integers(100,140)))\n",
                "    fx, fy, fw, fh = 20, 15, 60, 70\n",
                "    d.ellipse([fx, fy, fx+fw, fy+fh], fill=skin)\n",
                "    ey = fy + int(fh*.35)\n",
                "    for ex in [fx+int(fw*.25), fx+int(fw*.65)]:\n",
                "        d.ellipse([ex-4, ey-3, ex+4, ey+3], fill=(40,40,40))\n",
                "    nx, ny = fx+fw//2, fy+int(fh*.55)\n",
                "    d.ellipse([nx-3, ny-2, nx+3, ny+3], fill=(max(0,skin[0]-30), max(0,skin[1]-30), max(0,skin[2]-30)))\n",
                "    if mask:\n",
                "        mc = tuple(int(c) for c in rng.integers(0, 255, 3))\n",
                "        d.rectangle([fx+3, fy+int(fh*.5), fx+fw-3, fy+int(fh*.92)], fill=mc)\n",
                "        for row in range(fy+int(fh*.54), fy+int(fh*.90), 6):\n",
                "            d.line([(fx+5,row),(fx+fw-5,row)], fill=(min(255,mc[0]+40),min(255,mc[1]+40),min(255,mc[2]+40)), width=1)\n",
                "    else:\n",
                "        d.arc([fx+15, fy+int(fh*.7), fx+fw-15, fy+int(fh*.88)], 10, 170, fill=(180,80,80), width=2)\n",
                "    arr = np.array(img, dtype=np.float32) + rng.normal(0, 5, (100,100,3))\n",
                "    return Image.fromarray(np.clip(arr,0,255).astype(np.uint8))\n",
                "\n",
                "\n",
                "for cls, flag in [('with_mask', True), ('without_mask', False)]:\n",
                "    path = os.path.join(DATASET_DIR, cls)\n",
                "    if os.path.exists(path) and len(os.listdir(path)) >= N_IMAGES:\n",
                "        print(f'[SKIP] {cls} ‚Äî {len(os.listdir(path))} images already exist'); continue\n",
                "    os.makedirs(path, exist_ok=True)\n",
                "    offset = 0 if flag else N_IMAGES\n",
                "    for i in range(N_IMAGES):\n",
                "        make_face(flag, i+offset).save(os.path.join(path, f'{cls}_{i:04d}.png'))\n",
                "    print(f'[OK ] Generated {N_IMAGES} images ‚Üí {path}')\n",
                "\n",
                "# ‚îÄ‚îÄ Preview ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
                "fig, axes = plt.subplots(2, 5, figsize=(13, 5))\n",
                "fig.suptitle('Synthetic Dataset Samples', fontsize=14, fontweight='bold')\n",
                "for row, cls in enumerate(CATEGORIES):\n",
                "    files = os.listdir(os.path.join(DATASET_DIR, cls))[:5]\n",
                "    for col, fname in enumerate(files):\n",
                "        img = Image.open(os.path.join(DATASET_DIR, cls, fname))\n",
                "        axes[row][col].imshow(img)\n",
                "        axes[row][col].set_title(cls.replace('_',' ').title(), fontsize=8)\n",
                "        axes[row][col].axis('off')\n",
                "plt.tight_layout(); plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3 ¬∑ Custom Dataset Class & Data Loaders"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class MaskDataset(Dataset):\n",
                "    def __init__(self, paths, labels, transform=None):\n",
                "        self.paths = paths; self.labels = labels; self.transform = transform\n",
                "    def __len__(self): return len(self.paths)\n",
                "    def __getitem__(self, idx):\n",
                "        img = Image.open(self.paths[idx]).convert('RGB')\n",
                "        if self.transform: img = self.transform(img)\n",
                "        return img, self.labels[idx]\n",
                "\n",
                "\n",
                "# ‚îÄ‚îÄ Collect paths & labels ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
                "paths, labels = [], []\n",
                "for cls_idx, cat in enumerate(CATEGORIES):\n",
                "    folder = os.path.join(DATASET_DIR, cat)\n",
                "    for fname in os.listdir(folder):\n",
                "        fpath = os.path.join(folder, fname)\n",
                "        if os.path.isfile(fpath):\n",
                "            paths.append(fpath); labels.append(cls_idx)\n",
                "\n",
                "print(f'Total images : {len(paths)}')\n",
                "for i, cat in enumerate(CATEGORIES):\n",
                "    print(f'  {cat}: {labels.count(i)}')\n",
                "\n",
                "X_train, X_test, y_train, y_test = train_test_split(\n",
                "    paths, labels, test_size=0.20, stratify=labels, random_state=42\n",
                ")\n",
                "\n",
                "# ‚îÄ‚îÄ Transforms ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
                "MEAN, STD = [0.485,0.456,0.406], [0.229,0.224,0.225]\n",
                "\n",
                "train_tf = T.Compose([\n",
                "    T.Resize((IMG_SIZE, IMG_SIZE)),\n",
                "    T.RandomHorizontalFlip(),\n",
                "    T.RandomRotation(20),\n",
                "    T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
                "    T.ToTensor(),\n",
                "    T.Normalize(MEAN, STD)\n",
                "])\n",
                "val_tf = T.Compose([T.Resize((IMG_SIZE,IMG_SIZE)), T.ToTensor(), T.Normalize(MEAN,STD)])\n",
                "\n",
                "BATCH_SIZE   = 32\n",
                "train_loader = DataLoader(MaskDataset(X_train, y_train, train_tf), batch_size=BATCH_SIZE, shuffle=True,  num_workers=0)\n",
                "val_loader   = DataLoader(MaskDataset(X_test,  y_test,  val_tf),   batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
                "\n",
                "print(f'Train batches : {len(train_loader)}   Val batches : {len(val_loader)}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4 ¬∑ Build MobileNetV2 Model\n",
                "\n",
                "```\n",
                "MobileNetV2 (frozen, ImageNet weights)\n",
                "       ‚îÇ\n",
                "  Dropout (0.5)\n",
                "       ‚îÇ\n",
                "  Linear (1280 ‚Üí 128)\n",
                "       ‚îÇ\n",
                "    ReLU\n",
                "       ‚îÇ\n",
                "  Dropout (0.3)\n",
                "       ‚îÇ\n",
                "  Linear (128 ‚Üí 2)\n",
                "       ‚îÇ\n",
                "  Softmax         ‚Üê with_mask / without_mask\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "weights = MobileNet_V2_Weights.IMAGENET1K_V1\n",
                "model   = mobilenet_v2(weights=weights)\n",
                "\n",
                "# Freeze the entire base\n",
                "for param in model.parameters():\n",
                "    param.requires_grad = False\n",
                "\n",
                "# Replace classifier head\n",
                "in_feat = model.classifier[1].in_features\n",
                "model.classifier = nn.Sequential(\n",
                "    nn.Dropout(p=0.5),\n",
                "    nn.Linear(in_feat, 128),\n",
                "    nn.ReLU(),\n",
                "    nn.Dropout(p=0.3),\n",
                "    nn.Linear(128, 2)\n",
                ")\n",
                "model = model.to(DEVICE)\n",
                "\n",
                "total     = sum(p.numel() for p in model.parameters())\n",
                "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
                "print(f'Total params     : {total:,}')\n",
                "print(f'Trainable params : {trainable:,}  (head only)')\n",
                "print(f'Frozen params    : {total-trainable:,}  (MobileNetV2 base)')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5 ¬∑ Train"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "LR        = 1e-4\n",
                "EPOCHS    = 20\n",
                "PATIENCE  = 5\n",
                "MODEL_PATH= 'mask_detector.pth'\n",
                "\n",
                "criterion = nn.CrossEntropyLoss()\n",
                "optimizer = Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=LR)\n",
                "scheduler = StepLR(optimizer, step_size=5, gamma=0.5)\n",
                "\n",
                "history          = {'train_acc':[], 'val_acc':[], 'train_loss':[], 'val_loss':[]}\n",
                "best_val_acc     = 0.0\n",
                "patience_counter = 0\n",
                "\n",
                "print('Training ‚Ä¶\\n')\n",
                "for epoch in range(1, EPOCHS+1):\n",
                "    t0 = time.time()\n",
                "\n",
                "    # ‚îÄ‚îÄ train ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
                "    model.train()\n",
                "    r_loss = r_corr = r_n = 0\n",
                "    for imgs, lbls in train_loader:\n",
                "        imgs, lbls = imgs.to(DEVICE), lbls.to(DEVICE)\n",
                "        optimizer.zero_grad()\n",
                "        out  = model(imgs)\n",
                "        loss = criterion(out, lbls)\n",
                "        loss.backward(); optimizer.step()\n",
                "        r_loss += loss.item()*imgs.size(0)\n",
                "        r_corr += (out.argmax(1)==lbls).sum().item()\n",
                "        r_n    += imgs.size(0)\n",
                "    t_loss, t_acc = r_loss/r_n, r_corr/r_n\n",
                "\n",
                "    # ‚îÄ‚îÄ validate ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
                "    model.eval()\n",
                "    v_loss = v_corr = v_n = 0\n",
                "    with torch.no_grad():\n",
                "        for imgs, lbls in val_loader:\n",
                "            imgs, lbls = imgs.to(DEVICE), lbls.to(DEVICE)\n",
                "            out  = model(imgs)\n",
                "            v_loss += criterion(out, lbls).item()*imgs.size(0)\n",
                "            v_corr += (out.argmax(1)==lbls).sum().item()\n",
                "            v_n    += imgs.size(0)\n",
                "    v_loss, v_acc = v_loss/v_n, v_corr/v_n\n",
                "\n",
                "    history['train_acc'].append(t_acc);  history['val_acc'].append(v_acc)\n",
                "    history['train_loss'].append(t_loss); history['val_loss'].append(v_loss)\n",
                "\n",
                "    saved = ''\n",
                "    if v_acc > best_val_acc:\n",
                "        best_val_acc = v_acc\n",
                "        torch.save(model.state_dict(), MODEL_PATH)\n",
                "        saved = '  ‚úÖ saved'\n",
                "        patience_counter = 0\n",
                "    else:\n",
                "        patience_counter += 1\n",
                "\n",
                "    print(f'  Ep {epoch:02}/{EPOCHS}  '\n",
                "          f'loss={t_loss:.4f} acc={t_acc:.4f}  |  '\n",
                "          f'val_loss={v_loss:.4f} val_acc={v_acc:.4f}  '\n",
                "          f'({time.time()-t0:.1f}s){saved}')\n",
                "\n",
                "    if patience_counter >= PATIENCE:\n",
                "        print(f'\\nEarly stopping at epoch {epoch}'); break\n",
                "    scheduler.step()\n",
                "\n",
                "print(f'\\n‚úÖ Training done!  Best val_acc = {best_val_acc:.4f}  ‚Üí  {MODEL_PATH}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6 ¬∑ Training Curves"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "er = len(history['train_acc'])\n",
                "x  = range(1, er+1)\n",
                "\n",
                "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(13, 4))\n",
                "fig.suptitle('MobileNetV2 Transfer Learning ‚Äî Face Mask Detection', fontsize=14, fontweight='bold')\n",
                "\n",
                "ax1.plot(x, history['train_acc'], label='Train', lw=2, color='#4C72B0')\n",
                "ax1.plot(x, history['val_acc'],   label='Val',   lw=2, color='#DD8452', ls='--')\n",
                "ax1.set_title('Accuracy'); ax1.set_xlabel('Epoch'); ax1.legend(); ax1.grid(alpha=.3); ax1.set_ylim([0,1.05])\n",
                "\n",
                "ax2.plot(x, history['train_loss'], label='Train', lw=2, color='#4C72B0')\n",
                "ax2.plot(x, history['val_loss'],   label='Val',   lw=2, color='#DD8452', ls='--')\n",
                "ax2.set_title('Loss'); ax2.set_xlabel('Epoch'); ax2.legend(); ax2.grid(alpha=.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('training_plot.png', dpi=150)\n",
                "plt.show()\n",
                "print('Saved ‚Üí training_plot.png')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7 ¬∑ Evaluate"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
                "model.eval()\n",
                "\n",
                "all_preds, all_true = [], []\n",
                "with torch.no_grad():\n",
                "    for imgs, lbls in val_loader:\n",
                "        out = model(imgs.to(DEVICE))\n",
                "        all_preds.extend(out.argmax(1).cpu().numpy())\n",
                "        all_true.extend(lbls.numpy())\n",
                "\n",
                "print('Classification Report:')\n",
                "print(classification_report(all_true, all_preds, target_names=CATEGORIES))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8 ¬∑ Confusion Matrix"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "cm = confusion_matrix(all_true, all_preds)\n",
                "\n",
                "fig, ax = plt.subplots(figsize=(6,5))\n",
                "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
                "            xticklabels=CATEGORIES, yticklabels=CATEGORIES,\n",
                "            annot_kws={'size':14}, ax=ax)\n",
                "ax.set_xlabel('Predicted', fontsize=12)\n",
                "ax.set_ylabel('Actual', fontsize=12)\n",
                "ax.set_title('Confusion Matrix ‚Äî Face Mask Detector', fontsize=13, fontweight='bold')\n",
                "plt.tight_layout()\n",
                "plt.savefig('confusion_matrix.png', dpi=150)\n",
                "plt.show()\n",
                "print('Saved ‚Üí confusion_matrix.png')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9 ¬∑ Predict on a Random Test Sample"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "idx_pick = random.randint(0, len(X_test)-1)\n",
                "true_lbl = CATEGORIES[y_test[idx_pick]]\n",
                "\n",
                "img_pil  = Image.open(X_test[idx_pick]).convert('RGB')\n",
                "inp      = val_tf(img_pil).unsqueeze(0).to(DEVICE)\n",
                "\n",
                "with torch.no_grad():\n",
                "    out  = model(inp)\n",
                "    prob = torch.softmax(out, dim=1)[0]\n",
                "\n",
                "pred_idx  = prob.argmax().item()\n",
                "pred_lbl  = CATEGORIES[pred_idx]\n",
                "conf      = prob[pred_idx].item() * 100\n",
                "\n",
                "color = '#00b894' if pred_lbl == 'with_mask' else '#d63031'\n",
                "\n",
                "fig, ax = plt.subplots(figsize=(4,4))\n",
                "ax.imshow(img_pil)\n",
                "ax.set_title(f'Predicted : {pred_lbl}\\nTrue : {true_lbl}\\nConfidence : {conf:.1f}%',\n",
                "             fontsize=11, color=color, fontweight='bold')\n",
                "ax.axis('off')\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10 ¬∑ üé• Real-Time Webcam Demo\n",
                "\n",
                "> **Run this cell to start live mask detection from your webcam.**  \n",
                "> A separate OpenCV window will open. Press **Q** to quit."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import cv2\n",
                "\n",
                "LABELS_WC = ['Mask ‚úÖ', 'No Mask ‚ùå']\n",
                "COLORS_WC = [(0, 200, 0), (0, 0, 220)]   # BGR\n",
                "CONF_MIN  = 0.60\n",
                "\n",
                "# Load best saved model\n",
                "webcam_model = mobilenet_v2(weights=None)\n",
                "in_feat_wc   = webcam_model.classifier[1].in_features\n",
                "webcam_model.classifier = nn.Sequential(\n",
                "    nn.Dropout(0.5), nn.Linear(in_feat_wc, 128), nn.ReLU(),\n",
                "    nn.Dropout(0.3), nn.Linear(128, 2)\n",
                ")\n",
                "webcam_model.load_state_dict(torch.load(MODEL_PATH, map_location='cpu'))\n",
                "webcam_model.eval()\n",
                "\n",
                "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
                "\n",
                "cap = cv2.VideoCapture(0)\n",
                "if not cap.isOpened():\n",
                "    print('‚ö†Ô∏è  Cannot open webcam ‚Äî skipping demo.')\n",
                "else:\n",
                "    print('Webcam open ‚Äî press Q to quit.')\n",
                "    with torch.no_grad():\n",
                "        while True:\n",
                "            ret, frame = cap.read()\n",
                "            if not ret: break\n",
                "\n",
                "            gray  = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
                "            faces = face_cascade.detectMultiScale(gray, 1.1, 5, minSize=(60,60))\n",
                "\n",
                "            for (x,y,w,h) in faces:\n",
                "                roi  = cv2.cvtColor(frame[y:y+h, x:x+w], cv2.COLOR_BGR2RGB)\n",
                "                pil  = Image.fromarray(roi)\n",
                "                inp  = val_tf(pil).unsqueeze(0)\n",
                "                out  = webcam_model(inp)\n",
                "                prob = torch.softmax(out,1)[0]\n",
                "                idx2 = prob.argmax().item()\n",
                "                conf = prob[idx2].item()\n",
                "                if conf < CONF_MIN: continue\n",
                "                lbl   = f'{LABELS_WC[idx2]} ({conf*100:.1f}%)'\n",
                "                color = COLORS_WC[idx2]\n",
                "                cv2.rectangle(frame, (x,y), (x+w,y+h), color, 2)\n",
                "                cv2.rectangle(frame, (x,y-30), (x+w,y), color, -1)\n",
                "                cv2.putText(frame, lbl, (x+4,y-8), cv2.FONT_HERSHEY_SIMPLEX, .60, (255,255,255), 2)\n",
                "\n",
                "            cv2.putText(frame,'Face Mask Detector  |  Q to quit',(10,25),cv2.FONT_HERSHEY_SIMPLEX,.65,(200,200,200),1)\n",
                "            cv2.imshow('Face Mask Detector', frame)\n",
                "            if cv2.waitKey(1) & 0xFF == ord('q'): break\n",
                "\n",
                "    cap.release(); cv2.destroyAllWindows()\n",
                "    print('Webcam closed.')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Summary\n",
                "\n",
                "| Item | Value |\n",
                "|------|-------|\n",
                "| Framework | **PyTorch** |\n",
                "| Base model | MobileNetV2 (ImageNet) ‚Äî **frozen** |\n",
                "| Custom head | Dropout ‚Üí Linear(128, ReLU) ‚Üí Dropout ‚Üí Linear(2) |\n",
                "| Loss | CrossEntropyLoss |\n",
                "| Optimiser | Adam (lr=1e-4) |\n",
                "| Augmentation | HFlip, Rotation, ColorJitter |\n",
                "| Output | `mask_detector.pth` |\n",
                "\n",
                "### üí° To improve accuracy\n",
                "1. Use the **real dataset** (~4 k images) from https://github.com/chandrikadeb7/Face-Mask-Detection  \n",
                "2. **Unfreeze** MobileNetV2 top layers for fine-tuning with a very low lr (‚âà1e-5)  \n",
                "3. Increase `N_IMAGES` to 500+ if using synthetic data  \n",
                "4. Try **EfficientNet-B0** as the backbone"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.11.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}